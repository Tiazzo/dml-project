{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "from utils.build_dataset import build_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 VGG model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Import model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Identity()\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision import models \n",
    "\n",
    "vgg_model = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1)\n",
    "vgg_model.classifier = torch.nn.Identity()\n",
    "vgg_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Feature extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(batch, model):\n",
    "    with torch.no_grad():\n",
    "        features = model(batch)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def tokenize(text: str):\n",
    "    text = text.lower()\n",
    "\n",
    "    translator = str.maketrans(\"\",\"\", string.punctuation + string.digits + \"\\t\\r\\n\")\n",
    "    text = text.translate(translator)\n",
    "    text = \"<START> \" + text + \" <END>\"\n",
    "    return [x for x in text.split(\" \") if x != \"\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Build vocabulary over datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# clean captions and add <START> and <END> tokens\n",
    "def build_tokeninzed_vocabulary(dataloader):\n",
    "    vocabulary, mxlen = dict(), 0\n",
    "\n",
    "    for _, captions, img_name in dataloader:\n",
    "        for id in range(len(img_name)):\n",
    "            to_append = []\n",
    "            for i in range(5):\n",
    "                to_append.append(' '.join(tokenize(captions[i][id])))\n",
    "                mxlen = max(mxlen, len(to_append[-1].split(\" \")))\n",
    "\n",
    "            img_id = img_name[id].split(\"/\")[-1]\n",
    "            vocabulary[img_id] = to_append\n",
    "    \n",
    "    return vocabulary, mxlen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 Fill vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader, val_dataloader, test_dataloader, dataloader_train_small, dataloader_val_small = build_datasets()\n",
    "\n",
    "# train_voc, mxlen1 = build_tokeninzed_vocabulary(train_dataloader)\n",
    "# val_voc, mxlen2 = build_tokeninzed_vocabulary(val_dataloader)\n",
    "# test_voc, mxlen3 = build_tokeninzed_vocabulary(test_dataloader)\n",
    "small_train_voc, mxlen4 = build_tokeninzed_vocabulary(dataloader_train_small)\n",
    "small_val_voc, mxlen5 = build_tokeninzed_vocabulary(dataloader_val_small)\n",
    "mxlen = max(mxlen4, mxlen5)\n",
    "\n",
    "corpus = dict()\n",
    "corpus[\"<PAD>\"] = 0\n",
    "\n",
    "def fill_corpus(corpus, voc):\n",
    "    for lst in voc.values():\n",
    "        for sentence in lst:\n",
    "            for word in sentence.split(\" \"):\n",
    "                if word not in corpus:\n",
    "                    corpus[word] = len(corpus)\n",
    "\n",
    "fill_corpus(corpus, small_train_voc)\n",
    "fill_corpus(corpus, small_val_voc)\n",
    "\n",
    "# print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.4 Pad word with PAD tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_word(token, mxlen):\n",
    "    ret = [\"<PAD>\" for i in range(mxlen - len(token))]\n",
    "    for word in token: ret.append(word)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Load glove embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "def load_glove_embeddings(glove_file, embedding_dim=100):\n",
    "    embeddings_index = {}\n",
    "    \n",
    "    with open(glove_file, 'r', encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = vector\n",
    "    \n",
    "    print(f\"Loaded {len(embeddings_index)} word vectors.\")\n",
    "    return embeddings_index\n",
    "\n",
    "# Load GloVe embeddings (using 100-dimensional vectors as an example)\n",
    "glove_file = \"../data/glove/glove.6B.100d.txt\"\n",
    "glove_embeddings = load_glove_embeddings(glove_file, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 Create embedding mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_embedding_matrix(vocab, glove_embeddings, embedding_dim=100):\n",
    "    vocab_size = len(vocab)\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "    \n",
    "    for word, idx in vocab.items():\n",
    "        embedding_vector = glove_embeddings.get(word)\n",
    "        if embedding_vector is not None: # word found\n",
    "            embedding_matrix[idx] = embedding_vector\n",
    "        else: # word not found -> random init\n",
    "            embedding_matrix[idx] = np.random.normal(scale=0.6, size=(embedding_dim,))\n",
    "    \n",
    "    return embedding_matrix\n",
    "\n",
    "embedding_matrix = create_embedding_matrix(corpus, glove_embeddings, 100)\n",
    "embedding_matrix = torch.tensor(embedding_matrix, dtype=torch.float32) # to torch.tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCaptionModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, embedding_matrix, max_length, feature_size=4096, hidden_size=256):\n",
    "        super(ImageCaptionModel, self).__init__()\n",
    "\n",
    "        # Feature extractor per le immagini\n",
    "        self.image_fc = nn.Linear(25088, hidden_size)  # Da 4096 a 256 dimensioni\n",
    "        self.image_dropout = nn.Dropout(0.5)\n",
    "\n",
    "        # Modello di embedding per il testo\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
    "        self.embedding.weight.requires_grad = False  # Fissa l'embedding (non addestrabile)\n",
    "        \n",
    "        # LSTM per il testo\n",
    "        self.text_dropout = nn.Dropout(0.5)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size, batch_first=True)\n",
    "\n",
    "        # Decoder (combinazione di features immagine + testo)\n",
    "        self.fc1 = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, image_features, captions):\n",
    "        # Elaborazione delle feature delle immagini\n",
    "        img_feature = self.image_dropout(torch.relu(self.image_fc(image_features)))\n",
    "\n",
    "        # Elaborazione del testo (didascalie)\n",
    "        embedded_captions = self.embedding(captions)  # Ottieni embedding pre-addestrati\n",
    "        embedded_captions = self.text_dropout(embedded_captions)\n",
    "        \n",
    "        lstm_out, _ = self.lstm(embedded_captions)  # Output LSTM\n",
    "\n",
    "        # Prendi l'ultimo stato dell'LSTM (ultimo token della sequenza)\n",
    "        text_feature = lstm_out[:, -1, :]\n",
    "\n",
    "        # Concatenazione delle features dell'immagine e del testo\n",
    "        combined = torch.cat((img_feature, text_feature), dim=1)\n",
    "\n",
    "        # Passaggio attraverso il decoder fully connected\n",
    "        output = torch.relu(self.fc1(combined))\n",
    "        output = self.fc2(output)  # Previsione finale (vettore delle dimensioni del vocabolario)\n",
    "\n",
    "        return torch.log_softmax(output, dim=1)  # Usare log_softmax per la cross-entropy loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Training function definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_dataloader, criterion, optimizer, num_epochs=10):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for images, captions_list, _ in train_dataloader:\n",
    "            images = images.to(device)\n",
    "            \n",
    "            caption_tokenized = []\n",
    "            for captions in captions_list:\n",
    "                five_cap = []\n",
    "                for caption in captions:\n",
    "                    word_cap = []\n",
    "                    for word in pad_word(tokenize(caption), mxlen):\n",
    "                        word_cap.append(corpus[word])\n",
    "                    five_cap.append(word_cap)\n",
    "                caption_tokenized.append(five_cap)\n",
    "            \n",
    "            caption_tokenized = torch.tensor(caption_tokenized).to(device)\n",
    "            \n",
    "            image_features = extract_features(images, vgg_model)\n",
    "\n",
    "            for img_idx, captions in enumerate(caption_tokenized):\n",
    "                img_feature = image_features[img_idx].unsqueeze(0).to(device)\n",
    "\n",
    "                for caption in captions:\n",
    "                    input_caption = caption[:-1].to(device) # remove <END> token\n",
    "                    target_caption = caption[1:].to(device)  # skip <START> token\n",
    "\n",
    "                    optimizer.zero_grad() # init gradient zero\n",
    "\n",
    "                    output = model(img_feature, input_caption)\n",
    "                    loss = criterion(output, target_caption)\n",
    "\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_88368/489261451.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m cap_model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      5\u001b[0m vgg_model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 7\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcap_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataloader_train_small\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCrossEntropyLoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAdam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcap_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\n\u001b[1;32m     13\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[24], line 36\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_dataloader, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[1;32m     32\u001b[0m target_caption \u001b[38;5;241m=\u001b[39m caption[\u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# skip <START> token\u001b[39;00m\n\u001b[1;32m     34\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad() \u001b[38;5;66;03m# init gradient zero\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_feature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_caption\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, target_caption)\n\u001b[1;32m     39\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniconda3/envs/dml/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dml/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[23], line 33\u001b[0m, in \u001b[0;36mImageCaptionModel.forward\u001b[0;34m(self, image_features, captions)\u001b[0m\n\u001b[1;32m     30\u001b[0m lstm_out, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm(embedded_captions)  \u001b[38;5;66;03m# Output LSTM\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Prendi l'ultimo stato dell'LSTM (ultimo token della sequenza)\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m text_feature \u001b[38;5;241m=\u001b[39m \u001b[43mlstm_out\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Concatenazione delle features dell'immagine e del testo\u001b[39;00m\n\u001b[1;32m     36\u001b[0m combined \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((img_feature, text_feature), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 2"
     ]
    }
   ],
   "source": [
    "cap_model = ImageCaptionModel(len(corpus), embedding_dim=100, embedding_matrix=embedding_matrix, max_length=mxlen)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "cap_model.to(device)\n",
    "vgg_model.to(device)\n",
    "\n",
    "train_model(\n",
    "    cap_model, \n",
    "    dataloader_train_small, \n",
    "    nn.CrossEntropyLoss(), \n",
    "    torch.optim.Adam(cap_model.parameters(), lr=0.001), \n",
    "    num_epochs=3\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
