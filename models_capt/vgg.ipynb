{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "from utils.build_dataset import build_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 VGG model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Import model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Identity()\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision import models \n",
    "\n",
    "vgg_model = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1)\n",
    "vgg_model.classifier = torch.nn.Identity()\n",
    "vgg_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Feature extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(batch, model):\n",
    "    with torch.no_grad():\n",
    "        features = model(batch)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def tokenize(text: str):\n",
    "    text = text.lower()\n",
    "\n",
    "    translator = str.maketrans(\"\",\"\", string.punctuation + string.digits + \"\\t\\r\\n\")\n",
    "    text = text.translate(translator)\n",
    "    text = \"<START> \" + text + \" <END>\"\n",
    "    return [x for x in text.split(\" \") if x != \"\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Build vocabulary over datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# clean captions and add <START> and <END> tokens\n",
    "def build_tokeninzed_vocabulary(dataloader):\n",
    "    vocabulary, mxlen = dict(), 0\n",
    "\n",
    "    for _, captions, img_name in dataloader:\n",
    "        for id in range(len(img_name)):\n",
    "            to_append = []\n",
    "            for i in range(5):\n",
    "                to_append.append(' '.join(tokenize(captions[i][id])))\n",
    "                mxlen = max(mxlen, len(to_append[-1].split(\" \")))\n",
    "\n",
    "            img_id = img_name[id].split(\"/\")[-1]\n",
    "            vocabulary[img_id] = to_append\n",
    "    \n",
    "    return vocabulary, mxlen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 Fill vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader, val_dataloader, test_dataloader, dataloader_train_small, dataloader_val_small = build_datasets()\n",
    "\n",
    "# train_voc, mxlen1 = build_tokeninzed_vocabulary(train_dataloader)\n",
    "# val_voc, mxlen2 = build_tokeninzed_vocabulary(val_dataloader)\n",
    "# test_voc, mxlen3 = build_tokeninzed_vocabulary(test_dataloader)\n",
    "small_train_voc, mxlen4 = build_tokeninzed_vocabulary(dataloader_train_small)\n",
    "small_val_voc, mxlen5 = build_tokeninzed_vocabulary(dataloader_val_small)\n",
    "mxlen = max(mxlen4, mxlen5)\n",
    "\n",
    "corpus = dict()\n",
    "corpus[\"<PAD>\"] = 0\n",
    "\n",
    "def fill_corpus(corpus, voc):\n",
    "    for lst in voc.values():\n",
    "        for sentence in lst:\n",
    "            for word in sentence.split(\" \"):\n",
    "                if word not in corpus:\n",
    "                    corpus[word] = len(corpus)\n",
    "\n",
    "fill_corpus(corpus, small_train_voc)\n",
    "fill_corpus(corpus, small_val_voc)\n",
    "\n",
    "# print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.4 Pad word with PAD tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_word(token, mxlen):\n",
    "    ret = [\"<PAD>\" for i in range(mxlen - len(token))]\n",
    "    for word in token: ret.append(word)\n",
    "    return ret\n",
    "\n",
    "def pad_tensor(tensor, mxlen):\n",
    "    pad = torch.tensor([corpus[\"<PAD>\"] for i in range(mxlen - len(tensor))], dtype=torch.float32)\n",
    "    tensor = torch.cat((tensor, pad))\n",
    "    return tensor\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Load glove embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "def load_glove_embeddings(glove_file, embedding_dim=100):\n",
    "    embeddings_index = {}\n",
    "    \n",
    "    with open(glove_file, 'r', encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = vector\n",
    "    \n",
    "    print(f\"Loaded {len(embeddings_index)} word vectors.\")\n",
    "    return embeddings_index\n",
    "\n",
    "# Load GloVe embeddings (using 100-dimensional vectors as an example)\n",
    "glove_file = \"../data/glove/glove.6B.100d.txt\"\n",
    "glove_embeddings = load_glove_embeddings(glove_file, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 Create embedding mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_embedding_matrix(vocab, glove_embeddings, embedding_dim=100):\n",
    "    vocab_size = len(vocab)\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "    \n",
    "    for word, idx in vocab.items():\n",
    "        embedding_vector = glove_embeddings.get(word)\n",
    "        if embedding_vector is not None: # word found\n",
    "            embedding_matrix[idx] = embedding_vector\n",
    "        else: # word not found -> random init\n",
    "            embedding_matrix[idx] = np.random.normal(scale=0.6, size=(embedding_dim,))\n",
    "    \n",
    "    return embedding_matrix\n",
    "\n",
    "embedding_matrix = create_embedding_matrix(corpus, glove_embeddings, 100)\n",
    "embedding_matrix = torch.tensor(embedding_matrix, dtype=torch.float32) # to torch.tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCaptionModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, embedding_matrix, max_length, feature_size=4096, hidden_size=256):\n",
    "        super(ImageCaptionModel, self).__init__()\n",
    "\n",
    "        # Feature extractor per le immagini\n",
    "        self.image_fc = nn.Linear(25088, hidden_size)\n",
    "        self.image_dropout = nn.Dropout(0.5)\n",
    "\n",
    "        # Modello di embedding per il testo\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        \n",
    "        # LSTM per il testo\n",
    "        self.text_dropout = nn.Dropout(0.5)\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_size, batch_first=True)\n",
    "\n",
    "        # Decoder (combinazione di features immagine + testo)\n",
    "        self.fc1 = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, image_features, captions):\n",
    "        # Elaborazione delle feature delle immagini\n",
    "        img_feature = torch.relu(self.image_fc(image_features))\n",
    "        img_feature = self.image_dropout(img_feature)\n",
    "\n",
    "        # Elaborazione del testo (didascalie)\n",
    "        embedded_captions = self.embedding(captions)  # Ottieni embedding pre-addestrati\n",
    "        embedded_captions = self.text_dropout(embedded_captions)\n",
    "        \n",
    "        # print(embedded_captions.shape)\n",
    "\n",
    "        gru_out, _ = self.gru(embedded_captions)  # Output LSTM\n",
    "        text_feature = gru_out[-1, :].reshape(1, 256)\n",
    "\n",
    "        # print(img_feature.shape, text_feature.shape)\n",
    "\n",
    "        # Concatenazione delle features dell'immagine e del testo\n",
    "        combined = torch.cat((img_feature, text_feature), dim=1)\n",
    "\n",
    "        # Passaggio attraverso il decoder fully connected\n",
    "        output = torch.relu(self.fc1(combined))\n",
    "        output = self.fc2(output)  # Previsione finale (vettore delle dimensioni del vocabolario)\n",
    "\n",
    "        # print(torch.argmax(output))\n",
    "\n",
    "        return torch.log_softmax(output, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Training function definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_dataloader, criterion, optimizer, num_epochs=10):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for images, captions_list, _ in train_dataloader:\n",
    "            images = images.to(device)\n",
    "            \n",
    "            caption_tokenized = []\n",
    "            for captions in captions_list:\n",
    "                five_cap = []\n",
    "                for caption in captions:\n",
    "                    word_cap = []\n",
    "                    for word in pad_word(tokenize(caption), mxlen):\n",
    "                        word_cap.append(corpus[word])\n",
    "                    five_cap.append(word_cap)\n",
    "                caption_tokenized.append(five_cap)\n",
    "            \n",
    "            caption_tokenized = torch.tensor(caption_tokenized, dtype=torch.float32).to(device)\n",
    "            \n",
    "            image_features = extract_features(images, vgg_model)\n",
    "\n",
    "            for img_idx, captions in enumerate(caption_tokenized):\n",
    "                img_feature = image_features[img_idx].unsqueeze(0).to(device)\n",
    "\n",
    "                for caption in captions:\n",
    "                    # input_caption = caption[:-1].to(device) # remove <END> token\n",
    "                    target_caption = caption[1:].to(device)  # skip <START> token\n",
    "\n",
    "                    input_caption = torch.tensor([corpus[\"<START>\"]]).to(device)\n",
    "\n",
    "                    optimizer.zero_grad() # init gradient zero\n",
    "\n",
    "                    output = model(img_feature, input_caption)\n",
    "                    token = torch.argmax(output).item()\n",
    "                    while token != corpus[\"<END>\"] and len(input_caption) < mxlen-2:\n",
    "                        token = torch.tensor([token]).to(device)\n",
    "                        input_caption = torch.cat((input_caption, token), dim=0)\n",
    "                        output = model(img_feature, input_caption)\n",
    "                        token = torch.argmax(output).item()\n",
    "                    \n",
    "                    if input_caption[-1] != corpus[\"<END>\"]:\n",
    "                        token = corpus[\"<END>\"]\n",
    "                        token = torch.tensor([token]).to(device)\n",
    "                        input_caption = torch.cat((input_caption, token), dim=0)\n",
    "                    \n",
    "                    input_caption = pad_tensor(input_caption.to(\"cpu\"), mxlen-1).to(device)\n",
    "                    \n",
    "                    # print(output.shape, target_caption.shape)\n",
    "                    loss = criterion(input_caption, target_caption).requires_grad_()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23527/26975551.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Loss: 6385362005.4921875\n",
      "Epoch [2/3], Loss: 6321260414.0859375\n",
      "Epoch [3/3], Loss: 6376943304.50558\n"
     ]
    }
   ],
   "source": [
    "cap_model = ImageCaptionModel(len(corpus), embedding_dim=100, embedding_matrix=embedding_matrix, max_length=mxlen)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "cap_model.to(device)\n",
    "vgg_model.to(device)\n",
    "\n",
    "train_model(\n",
    "    cap_model, \n",
    "    dataloader_train_small, \n",
    "    nn.CrossEntropyLoss(), \n",
    "    torch.optim.Adam(cap_model.parameters(), lr=0.001), \n",
    "    num_epochs=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/train_small/1017675163.jpg\n",
      "<START> slightly kins cinema painter flinging slightly yelling travels roses earmuffs diagrams bookshelves potato soaks takes volkswagen bookshelves slightly volkswagen variously broncos featured their painter volkswagen slightly woodsy diagrams bookshelves faded banjo golfers yelling stretches variously painter cars slightly poodle beating yelling red stretches overpass bookshelves spreading shaped barbecue bookshelves yelling ritual diagrams roughhousing bookshelves ikea post takes roses slightly flinging roses bookshelves vases <END> "
     ]
    }
   ],
   "source": [
    "def produce_sentence(tokens):\n",
    "    for token in tokens:\n",
    "        print(list(corpus.keys())[list(corpus.values()).index(token)], end=\" \")\n",
    "\n",
    "\n",
    "img_id = dataloader_train_small.dataset[0][2]\n",
    "print(img_id)\n",
    "img = dataloader_train_small.dataset[0][0].unsqueeze(0).to(device)\n",
    "feat = extract_features(img, vgg_model)\n",
    "\n",
    "input_caption = torch.tensor([corpus[\"<START>\"]]).to(device)\n",
    "\n",
    "output = cap_model(feat, input_caption)\n",
    "token = torch.argmax(output).item()\n",
    "while token != corpus[\"<END>\"] and len(input_caption) < mxlen-2:\n",
    "    token = torch.tensor([token]).to(device)\n",
    "    input_caption = torch.cat((input_caption, token), dim=0)\n",
    "    output = cap_model(feat, input_caption)\n",
    "    token = torch.argmax(output).item()\n",
    "\n",
    "if input_caption[-1] != corpus[\"<END>\"]:\n",
    "    token = corpus[\"<END>\"]\n",
    "    token = torch.tensor([token]).to(device)\n",
    "    input_caption = torch.cat((input_caption, token), dim=0)\n",
    "\n",
    "produce_sentence(input_caption.to(\"cpu\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
