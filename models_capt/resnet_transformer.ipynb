{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torchvision.models as models\n",
    "import string\n",
    "import nltk\n",
    "import math\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions_file = '../data/train/captions_train.txt'\n",
    "\n",
    "captions_df = pd.read_table(captions_file, delimiter=',', header=None, names=['image', 'caption'])\n",
    "\n",
    "captions_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get only unique images\n",
    "unique_images = captions_df['image'].unique()\n",
    "\n",
    "\n",
    "# Split images\n",
    "train_images, test_images = train_test_split(unique_images, test_size=0.2, random_state=42)\n",
    "train_images, val_images = train_test_split(train_images, test_size=0.1, random_state=42)\n",
    "\n",
    "\n",
    "# DataFrame creation\n",
    "train_df = captions_df[captions_df['image'].isin(train_images)].reset_index(drop=True)\n",
    "train_df = train_df.dropna().reset_index(drop=True)\n",
    "val_df = captions_df[captions_df['image'].isin(val_images)].reset_index(drop=True)\n",
    "val_df = val_df.dropna().reset_index(drop=True)\n",
    "test_df = captions_df[captions_df['image'].isin(test_images)].reset_index(drop=True)\n",
    "test_df = test_df.dropna().reset_index(drop=True)\n",
    "\n",
    "print(f\"Numero di immagini nel set di addestramento: {len(train_images)}\")\n",
    "print(f\"Numero di immagini nel set di validazione: {len(val_images)}\")\n",
    "print(f\"Numero di immagini nel set di test: {len(test_images)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Vocabulary class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self, freq_threshold):\n",
    "        self.freq_threshold = freq_threshold\n",
    "        self.word2idx = {'<PAD>': 0, '<START>': 1, '<END>': 2, '<UNK>': 3}\n",
    "        self.idx2word = {0: '<PAD>', 1: '<START>', 2: '<END>', 3: '<UNK>'}\n",
    "        self.word_freq = {}\n",
    "        self.idx = 4\n",
    "        # self.translator = str.maketrans(\"\",\"\", string.punctuation + string.digits + \"\\t\\r\\n\")\n",
    "         \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        return nltk.tokenize.word_tokenize(text)\n",
    "\n",
    "    def build_vocabulary(self, sentence_list):\n",
    "        for sentence in sentence_list:\n",
    "            for word in self.tokenize(sentence):\n",
    "                if word not in self.word_freq:\n",
    "                    self.word_freq[word] = 1\n",
    "                else:\n",
    "                    self.word_freq[word] += 1\n",
    "\n",
    "                if self.word_freq[word] == self.freq_threshold:\n",
    "                    self.word2idx[word] = self.idx\n",
    "                    self.idx2word[self.idx] = word\n",
    "                    self.idx += 1\n",
    "\n",
    "    def numericalize(self, text):\n",
    "        tokenized_text = self.tokenize(text)\n",
    "        return [self.word2idx.get(word, self.word2idx['<UNK>']) for word in tokenized_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocabulary(freq_threshold=5)\n",
    "caption_list = train_df['caption'].tolist()\n",
    "vocab.build_vocabulary(caption_list)\n",
    "\n",
    "print(f\"Dimensione del vocabolario: {len(vocab)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_embeddings(glove_file):\n",
    "    embeddings = {}\n",
    "    with open(glove_file, 'r', encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings[word] = vector\n",
    "    print(f\"Caricati {len(embeddings)} vettori di embedding da GloVe.\")\n",
    "    return embeddings\n",
    "\n",
    "# Caricamento degli embeddings GloVe\n",
    "glove_file = '../data/glove/glove.6B.100d.txt'  # Sostituisci con il percorso corretto\n",
    "glove_embeddings = load_glove_embeddings(glove_file)\n",
    "\n",
    "def create_embedding_matrix(vocab, glove_embeddings, embedding_dim):\n",
    "    vocab_size = len(vocab)\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "    for word, idx in vocab.word2idx.items():\n",
    "        if word in glove_embeddings:\n",
    "            embedding_matrix[idx] = glove_embeddings[word]\n",
    "        else:\n",
    "            # Inizializza con un vettore casuale per le parole non trovate\n",
    "            embedding_matrix[idx] = np.random.normal(scale=0.6, size=(embedding_dim,))\n",
    "    return torch.tensor(embedding_matrix, dtype=torch.float32)\n",
    "\n",
    "embedding_dim = 100  # Deve corrispondere alla dimensione degli embeddings GloVe scaricati\n",
    "embedding_matrix = create_embedding_matrix(vocab, glove_embeddings, embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlickrDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_dir, vocab, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.vocab = vocab\n",
    "        self.transform = transform\n",
    "\n",
    "        # Creiamo una lista di coppie (immagine, didascalia)\n",
    "        self.image_ids = []\n",
    "        self.captions = []\n",
    "\n",
    "        grouped = dataframe.groupby('image')['caption'].apply(list).reset_index()\n",
    "\n",
    "        for idx in range(len(grouped)):\n",
    "            img_id = grouped.loc[idx, 'image']\n",
    "            captions = grouped.loc[idx, 'caption']\n",
    "            for cap in captions:\n",
    "                self.image_ids.append(img_id)\n",
    "                self.captions.append(cap)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.captions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        caption = self.captions[idx]\n",
    "        image_id = self.image_ids[idx]\n",
    "        image_path = os.path.join(self.image_dir, image_id)\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        numericalized_caption = [self.vocab.word2idx['<START>']]\n",
    "        numericalized_caption += self.vocab.numericalize(caption)\n",
    "        numericalized_caption.append(self.vocab.word2idx['<END>'])\n",
    "        numericalized_caption = torch.tensor(numericalized_caption)\n",
    "\n",
    "        return image, numericalized_caption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.1 Transformation Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],  # Valori standard per ImageNet\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    ),\n",
    "])\n",
    "\n",
    "tranf_only_tensor = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = '../data/train'\n",
    "\n",
    "train_dataset = FlickrDataset(train_df, image_dir, vocab, transform=transform)\n",
    "val_dataset = FlickrDataset(val_df, image_dir, vocab, transform=transform)\n",
    "test_dataset = FlickrDataset(test_df, image_dir, vocab, transform=transform)\n",
    "test_dataset_or = FlickrDataset(test_df, image_dir, vocab, transform=tranf_only_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    images = []\n",
    "    captions = []\n",
    "    for img, cap in batch:\n",
    "        images.append(img)\n",
    "        captions.append(cap)\n",
    "    images = torch.stack(images, dim=0)\n",
    "    captions = nn.utils.rnn.pad_sequence(captions, batch_first=True, padding_value=vocab.word2idx['<PAD>'])\n",
    "    return images, captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False, num_workers=2, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=2, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Encoder ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, d_model=512):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "        modules = list(resnet.children())[\n",
    "            :-2\n",
    "        ]  # Rimuovi gli ultimi due layer (avgpool e fc)\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        self.conv = nn.Conv2d(\n",
    "            2048, d_model, kernel_size=1\n",
    "        )  # Riduce la dimensione delle feature a d_model\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, images):\n",
    "        # Estrai le feature map\n",
    "        features = self.resnet(images)  # [batch_size, 2048, H, W]\n",
    "        features = self.conv(features)  # [batch_size, d_model, H, W]\n",
    "        features = features.flatten(2)  # [batch_size, d_model, H*W]\n",
    "        features = features.permute(2, 0, 1)  # [H*W, batch_size, d_model]\n",
    "        return features  # Sequence length = H*W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000, dropout=0.1):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        pe = pe.unsqueeze(1)  # [max_len, 1, d_model]\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[: x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Decoder with Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderWithTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_matrix, d_model=512, num_heads=8, num_layers=6, dropout=0.1):\n",
    "        super(DecoderWithTransformer, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        embed_size = embedding_matrix.size(1)\n",
    "        self.embed = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\n",
    "        self.embed_proj = nn.Linear(embed_size, d_model) if embed_size != d_model else nn.Identity()\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout=dropout)\n",
    "\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=d_model, nhead=num_heads, dropout=dropout\n",
    "        )\n",
    "        self.transformer_decoder = nn.TransformerDecoder(\n",
    "            decoder_layer, num_layers=num_layers\n",
    "        )\n",
    "\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, tgt, memory):\n",
    "        tgt_emb = self.embed(tgt)  # [seq_len_tgt, batch_size, embed_size]\n",
    "        tgt_emb = self.embed_proj(tgt_emb) * math.sqrt(self.d_model)  # [seq_len_tgt, batch_size, d_model]\n",
    "        tgt_emb = self.pos_encoder(tgt_emb)\n",
    "\n",
    "        tgt_mask = self.generate_square_subsequent_mask(tgt.size(0)).to(tgt.device)\n",
    "\n",
    "        output = self.transformer_decoder(tgt_emb, memory, tgt_mask=tgt_mask)\n",
    "        output = self.fc_out(output)  # [seq_len_tgt, batch_size, vocab_size]\n",
    "        return output\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = (\n",
    "            mask.float()\n",
    "            .masked_fill(mask == 0, float(\"-inf\"))\n",
    "            .masked_fill(mask == 1, float(0.0))\n",
    "        )\n",
    "        return mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "d_model = 512 \n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "\n",
    "encoder = EncoderCNN(d_model).to(device)\n",
    "decoder = DecoderWithTransformer(vocab_size, embedding_matrix, d_model, num_heads, num_layers).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(encoder, decoder, criterion, optimizer, dataloader, valdataloader, num_epochs):\n",
    "    train_losses, val_losses = [], []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "\n",
    "        # Training phase\n",
    "        total_train_loss = 0\n",
    "        for i, (images, captions) in enumerate(dataloader):\n",
    "            images = images.to(device)\n",
    "            captions = captions.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            memory = encoder(images)  # [seq_len_src, batch_size, d_model]\n",
    "\n",
    "            # Prepare input and target for decoder\n",
    "            tgt_input = captions[:, :-1].permute(1, 0)  # [seq_len_tgt, batch_size]\n",
    "            tgt_output = captions[:, 1:].permute(1, 0)  # [seq_len_tgt, batch_size]\n",
    "\n",
    "            outputs = decoder(tgt_input, memory)  # [seq_len_tgt, batch_size, vocab_size]\n",
    "\n",
    "            # Loss computation\n",
    "            loss = criterion(outputs.reshape(-1, outputs.size(-1)), tgt_output.reshape(-1))\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i}/{len(dataloader)}], Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        average_train_loss = total_train_loss / len(dataloader)\n",
    "        train_losses.append(average_train_loss)\n",
    "\n",
    "        # Validation phase\n",
    "        encoder.eval()\n",
    "        decoder.eval()\n",
    "        total_val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for i, (images, captions) in enumerate(valdataloader):\n",
    "                images = images.to(device)\n",
    "                captions = captions.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                memory = encoder(images)\n",
    "                tgt_input = captions[:, :-1].permute(1, 0)\n",
    "                tgt_output = captions[:, 1:].permute(1, 0)\n",
    "                outputs = decoder(tgt_input, memory)\n",
    "\n",
    "                # Loss computation\n",
    "                loss = criterion(outputs.reshape(-1, outputs.size(-1)), tgt_output.reshape(-1))\n",
    "                total_val_loss += loss.item()\n",
    "\n",
    "        average_val_loss = total_val_loss / len(valdataloader)\n",
    "        val_losses.append(average_val_loss)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {average_train_loss:.4f}, Validation Loss: {average_val_loss:.4f}\")\n",
    "\n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab.word2idx['<PAD>'])\n",
    "params = list(decoder.parameters()) + list(encoder.parameters())\n",
    "optimizer = torch.optim.Adam(params, lr=learning_rate)\n",
    "\n",
    "train_losses, val_losses = train_model(\n",
    "    encoder, decoder, criterion, optimizer, train_loader, val_loader, num_epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.title('Training and Validation Loss over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Caption generation and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption_beam_search(encoder, decoder, image, vocab, beam_size=3, max_length=20):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    with torch.no_grad():\n",
    "        # Preprocessamento dell'immagine\n",
    "        if len(image.shape) == 3:\n",
    "            image = image.unsqueeze(0)  # Aggiunge la dimensione batch se necessario\n",
    "        image = image.to(device)\n",
    "\n",
    "        # Ottieni le feature dall'encoder\n",
    "        memory = encoder(image)  # [seq_len_src, batch_size=1, d_model]\n",
    "\n",
    "        # Inizializza le sequenze di didascalie e punteggi\n",
    "        sequences = [[vocab.word2idx['<START>']]]\n",
    "        scores = torch.zeros(1).to(device)\n",
    "\n",
    "        for _ in range(max_length):\n",
    "            all_candidates = []\n",
    "            for i in range(len(sequences)):\n",
    "                seq = sequences[i]\n",
    "                score = scores[i]\n",
    "\n",
    "                tgt_input = torch.tensor(seq).unsqueeze(1).to(device)  # [seq_len_tgt, 1]\n",
    "                output = decoder(tgt_input, memory)  # [seq_len_tgt, 1, vocab_size]\n",
    "                output = output[-1, 0, :]  # Prendiamo l'ultimo token\n",
    "                probs = torch.log_softmax(output, dim=-1)\n",
    "                top_k_probs, top_k_idx = probs.topk(beam_size)\n",
    "\n",
    "                for j in range(beam_size):\n",
    "                    candidate = seq + [top_k_idx[j].item()]\n",
    "                    candidate_score = score + top_k_probs[j]\n",
    "                    all_candidates.append((candidate_score, candidate))\n",
    "\n",
    "            ordered = sorted(all_candidates, key=lambda tup: tup[0], reverse=True)\n",
    "            sequences = [candidate for _, candidate in ordered[:beam_size]]\n",
    "            scores = torch.tensor([score for score, _ in ordered[:beam_size]]).to(device)\n",
    "\n",
    "        # Prendi la sequenza migliore\n",
    "        best_seq = sequences[0]\n",
    "        sampled_caption = [vocab.idx2word[idx] for idx in best_seq[1:] if idx != vocab.word2idx['<END>']]\n",
    "        sentence = ' '.join(sampled_caption)\n",
    "    return sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# image picking\n",
    "idx = random.randint(0, len(test_dataset))\n",
    "test_image, _ = test_dataset[idx]\n",
    "print_image, _ = test_dataset_or[idx]\n",
    "plt.imshow(print_image.permute(1, 2, 0))\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# caption generation\n",
    "caption = generate_caption(encoder, decoder, test_image, vocab)\n",
    "caption = ' '.join(caption.split()[:-1])\n",
    "print(\"Didascalia Generata:\", caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean(l): return sum(l)/len(l)\n",
    "\n",
    "lt = len(train_losses)\n",
    "lv = len(val_losses)\n",
    "step = 100\n",
    "t_l = [mean(train_losses[i:min(i+lt//step, lt)]) for i in range(0, lt, lt//step)]\n",
    "v_l = [mean(val_losses[i:min(i+lv//step, lv)]) for i in range(0, lv, lv//step)]\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(t_l, label='Training Loss')\n",
    "plt.plot(v_l, label='Validation Loss')\n",
    "plt.title('ResNet + RNN & Attention -- Training vs Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.xticks(ticks=range(0, len(t_l), 10), labels=range(0, 11))\n",
    "plt.legend()\n",
    "# plt.show()\n",
    "plt.savefig('../img/resnet_rnn_att.pdf', format='pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Chekpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# save model\n",
    "# torch.save(\n",
    "#     {\n",
    "#         \"encoder_state_dict\": encoder.state_dict(),\n",
    "#         \"decoder_state_dict\": decoder.state_dict(),\n",
    "#         \"train_losses\": train_losses,\n",
    "#         \"val_losses\": val_losses,\n",
    "#     },\n",
    "#     \"../ckpt/resnet_rnn_att.ckpt\",\n",
    "# )\n",
    "\n",
    "checkpoint = torch.load(\"../ckpt/resnet_rnn_att.ckpt\")\n",
    "\n",
    "encoder = EncoderCNN(embed_size)\n",
    "encoder.load_state_dict(checkpoint[\"encoder_state_dict\"])\n",
    "encoder = encoder.to(device)\n",
    "\n",
    "decoder = DecoderWithAttention(attention_dim, embed_size, hidden_size, vocab_size).to(device)\n",
    "decoder.load_state_dict(checkpoint[\"decoder_state_dict\"])\n",
    "decoder = decoder.to(device)\n",
    "\n",
    "train_losses = checkpoint[\"train_losses\"]\n",
    "val_losses = checkpoint[\"val_losses\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
