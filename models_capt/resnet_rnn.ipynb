{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torchvision.models as models\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions_file = '../data/train/captions_train.txt'\n",
    "\n",
    "captions_df = pd.read_table(captions_file, delimiter=',', header=None, names=['image', 'caption'])\n",
    "\n",
    "captions_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get only unique images\n",
    "unique_images = captions_df['image'].unique()\n",
    "\n",
    "\n",
    "# Split images\n",
    "train_images, test_images = train_test_split(unique_images, test_size=0.2, random_state=42)\n",
    "train_images, val_images = train_test_split(train_images, test_size=0.1, random_state=42)\n",
    "\n",
    "\n",
    "# DataFrame creation\n",
    "train_df = captions_df[captions_df['image'].isin(train_images)].reset_index(drop=True)\n",
    "train_df = train_df.dropna().reset_index(drop=True)\n",
    "val_df = captions_df[captions_df['image'].isin(val_images)].reset_index(drop=True)\n",
    "val_df = val_df.dropna().reset_index(drop=True)\n",
    "test_df = captions_df[captions_df['image'].isin(test_images)].reset_index(drop=True)\n",
    "test_df = test_df.dropna().reset_index(drop=True)\n",
    "\n",
    "print(f\"Numero di immagini nel set di addestramento: {len(train_images)}\")\n",
    "print(f\"Numero di immagini nel set di validazione: {len(val_images)}\")\n",
    "print(f\"Numero di immagini nel set di test: {len(test_images)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Vocabulary class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self, freq_threshold):\n",
    "        self.freq_threshold = freq_threshold\n",
    "        self.word2idx = {'<PAD>': 0, '<START>': 1, '<END>': 2, '<UNK>': 3}\n",
    "        self.idx2word = {0: '<PAD>', 1: '<START>', 2: '<END>', 3: '<UNK>'}\n",
    "        self.word_freq = {}\n",
    "        self.idx = 4\n",
    "        # self.translator = str.maketrans(\"\",\"\", string.punctuation + string.digits + \"\\t\\r\\n\")\n",
    "         \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        return nltk.tokenize.word_tokenize(text)\n",
    "\n",
    "    def build_vocabulary(self, sentence_list):\n",
    "        for sentence in sentence_list:\n",
    "            for word in self.tokenize(sentence):\n",
    "                if word not in self.word_freq:\n",
    "                    self.word_freq[word] = 1\n",
    "                else:\n",
    "                    self.word_freq[word] += 1\n",
    "\n",
    "                if self.word_freq[word] == self.freq_threshold:\n",
    "                    self.word2idx[word] = self.idx\n",
    "                    self.idx2word[self.idx] = word\n",
    "                    self.idx += 1\n",
    "\n",
    "    def numericalize(self, text):\n",
    "        tokenized_text = self.tokenize(text)\n",
    "        return [self.word2idx.get(word, self.word2idx['<UNK>']) for word in tokenized_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocabulary(freq_threshold=5)\n",
    "caption_list = train_df['caption'].tolist()\n",
    "vocab.build_vocabulary(caption_list)\n",
    "\n",
    "print(f\"Dimensione del vocabolario: {len(vocab)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_embeddings(glove_file):\n",
    "    embeddings = {}\n",
    "    with open(glove_file, 'r', encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings[word] = vector\n",
    "    print(f\"Caricati {len(embeddings)} vettori di embedding da GloVe.\")\n",
    "    return embeddings\n",
    "\n",
    "# Caricamento degli embeddings GloVe\n",
    "glove_file = '../data/glove/glove.6B.100d.txt'  # Sostituisci con il percorso corretto\n",
    "glove_embeddings = load_glove_embeddings(glove_file)\n",
    "\n",
    "def create_embedding_matrix(vocab, glove_embeddings, embedding_dim):\n",
    "    vocab_size = len(vocab)\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "    for word, idx in vocab.word2idx.items():\n",
    "        if word in glove_embeddings:\n",
    "            embedding_matrix[idx] = glove_embeddings[word]\n",
    "        else:\n",
    "            # Inizializza con un vettore casuale per le parole non trovate\n",
    "            embedding_matrix[idx] = np.random.normal(scale=0.6, size=(embedding_dim,))\n",
    "    return torch.tensor(embedding_matrix, dtype=torch.float32)\n",
    "\n",
    "embedding_dim = 100  # Deve corrispondere alla dimensione degli embeddings GloVe scaricati\n",
    "embedding_matrix = create_embedding_matrix(vocab, glove_embeddings, embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlickrDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_dir, vocab, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.vocab = vocab\n",
    "        self.transform = transform\n",
    "\n",
    "        # Creiamo una lista di coppie (immagine, didascalia)\n",
    "        self.image_ids = []\n",
    "        self.captions = []\n",
    "\n",
    "        grouped = dataframe.groupby('image')['caption'].apply(list).reset_index()\n",
    "\n",
    "        for idx in range(len(grouped)):\n",
    "            img_id = grouped.loc[idx, 'image']\n",
    "            captions = grouped.loc[idx, 'caption']\n",
    "            for cap in captions:\n",
    "                self.image_ids.append(img_id)\n",
    "                self.captions.append(cap)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.captions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        caption = self.captions[idx]\n",
    "        image_id = self.image_ids[idx]\n",
    "        image_path = os.path.join(self.image_dir, image_id)\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        numericalized_caption = [self.vocab.word2idx['<START>']]\n",
    "        numericalized_caption += self.vocab.numericalize(caption)\n",
    "        numericalized_caption.append(self.vocab.word2idx['<END>'])\n",
    "        numericalized_caption = torch.tensor(numericalized_caption)\n",
    "\n",
    "        return image, numericalized_caption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.1 Transformation Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],  # Valori standard per ImageNet\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    ),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = '../data/train'\n",
    "\n",
    "train_dataset = FlickrDataset(train_df, image_dir, vocab, transform=transform)\n",
    "val_dataset = FlickrDataset(val_df, image_dir, vocab, transform=transform)\n",
    "test_dataset = FlickrDataset(test_df, image_dir, vocab, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    images = []\n",
    "    captions = []\n",
    "    for img, cap in batch:\n",
    "        images.append(img)\n",
    "        captions.append(cap)\n",
    "    images = torch.stack(images, dim=0)\n",
    "    captions = nn.utils.rnn.pad_sequence(captions, batch_first=True, padding_value=vocab.word2idx['<PAD>'])\n",
    "    return images, captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False, num_workers=2, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=2, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Encoder (ResNet50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "        for param in resnet.parameters():\n",
    "            param.requires_grad = False  \n",
    "        modules = list(resnet.children())[:-1] \n",
    "\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        self.fc = nn.Linear(resnet.fc.in_features, embed_size)\n",
    "        self.bn = nn.BatchNorm1d(embed_size, momentum=0.01)\n",
    "\n",
    "    def forward(self, images):\n",
    "        features = self.resnet(images)\n",
    "        features = features.reshape(features.size(0), -1)\n",
    "        features = self.bn(self.fc(features))\n",
    "        return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Decoder (LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, embedding_matrix, num_layers=1):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.embed = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "        # init hidden state\n",
    "        self.init_h = nn.Linear(embed_size, hidden_size)\n",
    "        self.init_c = nn.Linear(embed_size, hidden_size)\n",
    "    \n",
    "    def forward(self, features, captions):\n",
    "        embeddings = self.embed(captions)  # [batch_size, seq_len, embed_size]\n",
    "        \n",
    "        # init hidden state with features\n",
    "        h0 = self.init_h(features).unsqueeze(0)  # [num_layers, batch_size, hidden_size]\n",
    "        c0 = self.init_c(features).unsqueeze(0)  # [num_layers, batch_size, hidden_size]\n",
    "        \n",
    "        # Passa gli embeddings e lo stato nascosto all'LSTM\n",
    "        hiddens, _ = self.lstm(embeddings, (h0, c0))\n",
    "        outputs = self.fc(hiddens)\n",
    "        return outputs\n",
    "    \n",
    "    def sample(self, features, max_len=20):\n",
    "        \"Genera una didascalia data l'immagine\"\n",
    "        sampled_ids = []\n",
    "        \n",
    "        inputs = features.unsqueeze(1)  # [batch_size, 1, embed_size]\n",
    "        h0 = self.init_h(features).unsqueeze(0)\n",
    "        c0 = self.init_c(features).unsqueeze(0)\n",
    "        \n",
    "        states = (h0, c0)\n",
    "        for _ in range(max_len):\n",
    "            hiddens, states = self.lstm(inputs, states)\n",
    "            outputs = self.fc(hiddens.squeeze(1))\n",
    "            _, predicted = outputs.max(1)\n",
    "            sampled_ids.append(predicted.item())\n",
    "            inputs = self.embed(predicted)\n",
    "            inputs = inputs.unsqueeze(1)\n",
    "            if predicted.item() == vocab.word2idx['<END>']:\n",
    "                break\n",
    "        return sampled_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Embedding matrix function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_matrix(vocab, glove_embeddings, embedding_dim):\n",
    "    vocab_size = len(vocab)\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "    for word, idx in vocab.word2idx.items():\n",
    "        if word in glove_embeddings:\n",
    "            embedding_matrix[idx] = glove_embeddings[word]\n",
    "        else:\n",
    "            # Inizializza con un vettore casuale per le parole non trovate\n",
    "            embedding_matrix[idx] = np.random.normal(scale=0.6, size=(embedding_dim,))\n",
    "    return torch.tensor(embedding_matrix, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = embedding_dim  # Deve corrispondere alla dimensione degli embeddings GloVe\n",
    "hidden_size = 512\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "encoder = EncoderCNN(embed_size).to(device)\n",
    "decoder = DecoderRNN(embed_size, hidden_size, vocab_size, embedding_matrix).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 3e-4\n",
    "\n",
    "for param in decoder.embed.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "decoder_optimizer = torch.optim.Adam([\n",
    "    {'params': decoder.lstm.parameters()},\n",
    "    {'params': decoder.fc.parameters()}\n",
    "], lr=3e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(encoder, decoder, criterion, decoder_optimizer, dataloader, num_epochs):\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "\n",
    "    total_step = len(dataloader)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (images, captions) in enumerate(dataloader):\n",
    "            images = images.to(device)\n",
    "            captions = captions.to(device)\n",
    "\n",
    "            # Imposta i gradienti a zero\n",
    "            decoder_optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            features = encoder(images)\n",
    "            outputs = decoder(features, captions[:, :-1]) \n",
    "\n",
    "            # Calcolo della perdita\n",
    "            loss = criterion(outputs.reshape(-1, outputs.size(2)), captions[:, 1:].reshape(-1))\n",
    "\n",
    "            # Backward pass e ottimizzazione\n",
    "            loss.backward()\n",
    "            decoder_optimizer.step()\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i}/{total_step}], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab.word2idx['<PAD>'])\n",
    "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "train_model(encoder, decoder, criterion, decoder_optimizer, train_loader, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Caption generation and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption(encoder, decoder, image, vocab, max_length=20):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    with torch.no_grad():\n",
    "        image = image.to(device)\n",
    "        feature = encoder(image.unsqueeze(0))\n",
    "        sampled_ids = decoder.sample(feature, max_length)\n",
    "        sampled_caption = []\n",
    "        for word_id in sampled_ids:\n",
    "            word = vocab.idx2word[word_id]\n",
    "            if word == '<END>':\n",
    "                break\n",
    "            sampled_caption.append(word)\n",
    "        sentence = ' '.join(sampled_caption)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image picking\n",
    "test_image, _ = test_dataset[37]\n",
    "plt.imshow(test_image.permute(1, 2, 0))\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# caption generation\n",
    "caption = generate_caption(encoder, decoder, test_image, vocab)\n",
    "print(\"Didascalia Generata:\", caption)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
