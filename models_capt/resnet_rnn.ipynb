{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /home/white/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torchvision.models as models\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000092795.jpg</td>\n",
       "      <td>Two young guys with shaggy hair look at their...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000092795.jpg</td>\n",
       "      <td>Two young , White males are outside near many...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000092795.jpg</td>\n",
       "      <td>Two men in green shirts are standing in a yard .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000092795.jpg</td>\n",
       "      <td>A man in a blue shirt standing in a garden .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000092795.jpg</td>\n",
       "      <td>Two friends enjoy time spent together .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            image                                            caption\n",
       "0  1000092795.jpg   Two young guys with shaggy hair look at their...\n",
       "1  1000092795.jpg   Two young , White males are outside near many...\n",
       "2  1000092795.jpg   Two men in green shirts are standing in a yard .\n",
       "3  1000092795.jpg       A man in a blue shirt standing in a garden .\n",
       "4  1000092795.jpg            Two friends enjoy time spent together ."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captions_file = '../data/train/captions_train.txt'\n",
    "\n",
    "captions_df = pd.read_table(captions_file, delimiter=',', header=None, names=['image', 'caption'])\n",
    "\n",
    "captions_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero di immagini nel set di addestramento: 16018\n",
      "Numero di immagini nel set di validazione: 1780\n",
      "Numero di immagini nel set di test: 4450\n"
     ]
    }
   ],
   "source": [
    "# Get only unique images\n",
    "unique_images = captions_df['image'].unique()\n",
    "\n",
    "\n",
    "# Split images\n",
    "train_images, test_images = train_test_split(unique_images, test_size=0.2, random_state=42)\n",
    "train_images, val_images = train_test_split(train_images, test_size=0.1, random_state=42)\n",
    "\n",
    "\n",
    "# DataFrame creation\n",
    "train_df = captions_df[captions_df['image'].isin(train_images)].reset_index(drop=True)\n",
    "train_df = train_df.dropna().reset_index(drop=True)\n",
    "val_df = captions_df[captions_df['image'].isin(val_images)].reset_index(drop=True)\n",
    "val_df = val_df.dropna().reset_index(drop=True)\n",
    "test_df = captions_df[captions_df['image'].isin(test_images)].reset_index(drop=True)\n",
    "test_df = test_df.dropna().reset_index(drop=True)\n",
    "\n",
    "print(f\"Numero di immagini nel set di addestramento: {len(train_images)}\")\n",
    "print(f\"Numero di immagini nel set di validazione: {len(val_images)}\")\n",
    "print(f\"Numero di immagini nel set di test: {len(test_images)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Vocabulary class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self, freq_threshold):\n",
    "        self.freq_threshold = freq_threshold\n",
    "        self.word2idx = {'<PAD>': 0, '<START>': 1, '<END>': 2, '<UNK>': 3}\n",
    "        self.idx2word = {0: '<PAD>', 1: '<START>', 2: '<END>', 3: '<UNK>'}\n",
    "        self.word_freq = {}\n",
    "        self.idx = 4\n",
    "        # self.translator = str.maketrans(\"\",\"\", string.punctuation + string.digits + \"\\t\\r\\n\")\n",
    "         \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        return nltk.tokenize.word_tokenize(text)\n",
    "\n",
    "    def build_vocabulary(self, sentence_list):\n",
    "        for sentence in sentence_list:\n",
    "            for word in self.tokenize(sentence):\n",
    "                if word not in self.word_freq:\n",
    "                    self.word_freq[word] = 1\n",
    "                else:\n",
    "                    self.word_freq[word] += 1\n",
    "\n",
    "                if self.word_freq[word] == self.freq_threshold:\n",
    "                    self.word2idx[word] = self.idx\n",
    "                    self.idx2word[self.idx] = word\n",
    "                    self.idx += 1\n",
    "\n",
    "    def numericalize(self, text):\n",
    "        tokenized_text = self.tokenize(text)\n",
    "        return [self.word2idx.get(word, self.word2idx['<UNK>']) for word in tokenized_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensione del vocabolario: 5900\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocabulary(freq_threshold=5)\n",
    "caption_list = train_df['caption'].tolist()\n",
    "vocab.build_vocabulary(caption_list)\n",
    "\n",
    "print(f\"Dimensione del vocabolario: {len(vocab)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caricati 400000 vettori di embedding da GloVe.\n"
     ]
    }
   ],
   "source": [
    "def load_glove_embeddings(glove_file):\n",
    "    embeddings = {}\n",
    "    with open(glove_file, 'r', encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings[word] = vector\n",
    "    print(f\"Caricati {len(embeddings)} vettori di embedding da GloVe.\")\n",
    "    return embeddings\n",
    "\n",
    "# Caricamento degli embeddings GloVe\n",
    "glove_file = '../data/glove/glove.6B.100d.txt'  # Sostituisci con il percorso corretto\n",
    "glove_embeddings = load_glove_embeddings(glove_file)\n",
    "\n",
    "def create_embedding_matrix(vocab, glove_embeddings, embedding_dim):\n",
    "    vocab_size = len(vocab)\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "    for word, idx in vocab.word2idx.items():\n",
    "        if word in glove_embeddings:\n",
    "            embedding_matrix[idx] = glove_embeddings[word]\n",
    "        else:\n",
    "            # Inizializza con un vettore casuale per le parole non trovate\n",
    "            embedding_matrix[idx] = np.random.normal(scale=0.6, size=(embedding_dim,))\n",
    "    return torch.tensor(embedding_matrix, dtype=torch.float32)\n",
    "\n",
    "embedding_dim = 100  # Deve corrispondere alla dimensione degli embeddings GloVe scaricati\n",
    "embedding_matrix = create_embedding_matrix(vocab, glove_embeddings, embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlickrDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_dir, vocab, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.vocab = vocab\n",
    "        self.transform = transform\n",
    "\n",
    "        # Creiamo una lista di coppie (immagine, didascalia)\n",
    "        self.image_ids = []\n",
    "        self.captions = []\n",
    "\n",
    "        grouped = dataframe.groupby('image')['caption'].apply(list).reset_index()\n",
    "\n",
    "        for idx in range(len(grouped)):\n",
    "            img_id = grouped.loc[idx, 'image']\n",
    "            captions = grouped.loc[idx, 'caption']\n",
    "            for cap in captions:\n",
    "                self.image_ids.append(img_id)\n",
    "                self.captions.append(cap)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.captions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        caption = self.captions[idx]\n",
    "        image_id = self.image_ids[idx]\n",
    "        image_path = os.path.join(self.image_dir, image_id)\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        numericalized_caption = [self.vocab.word2idx['<START>']]\n",
    "        numericalized_caption += self.vocab.numericalize(caption)\n",
    "        numericalized_caption.append(self.vocab.word2idx['<END>'])\n",
    "        numericalized_caption = torch.tensor(numericalized_caption)\n",
    "\n",
    "        return image, numericalized_caption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.1 Transformation Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],  # Valori standard per ImageNet\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    ),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = '../data/train'\n",
    "\n",
    "train_dataset = FlickrDataset(train_df, image_dir, vocab, transform=transform)\n",
    "val_dataset = FlickrDataset(val_df, image_dir, vocab, transform=transform)\n",
    "test_dataset = FlickrDataset(test_df, image_dir, vocab, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    images = []\n",
    "    captions = []\n",
    "    for img, cap in batch:\n",
    "        images.append(img)\n",
    "        captions.append(cap)\n",
    "    images = torch.stack(images, dim=0)\n",
    "    captions = nn.utils.rnn.pad_sequence(captions, batch_first=True, padding_value=vocab.word2idx['<PAD>'])\n",
    "    return images, captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False, num_workers=2, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=2, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Encoder (ResNet50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "        for param in resnet.parameters():\n",
    "            param.requires_grad = False  \n",
    "        modules = list(resnet.children())[:-1] \n",
    "\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        self.fc = nn.Linear(resnet.fc.in_features, embed_size)\n",
    "        self.bn = nn.BatchNorm1d(embed_size, momentum=0.01)\n",
    "\n",
    "    def forward(self, images):\n",
    "        features = self.resnet(images)\n",
    "        features = features.reshape(features.size(0), -1)\n",
    "        features = self.bn(self.fc(features))\n",
    "        return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Decoder (LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, embedding_matrix, num_layers=1):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.embed = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "        # init hidden state\n",
    "        self.init_h = nn.Linear(embed_size, hidden_size)\n",
    "        self.init_c = nn.Linear(embed_size, hidden_size)\n",
    "    \n",
    "    def forward(self, features, captions):\n",
    "        embeddings = self.embed(captions)  # [batch_size, seq_len, embed_size]\n",
    "        \n",
    "        # init hidden state with features\n",
    "        h0 = self.init_h(features).unsqueeze(0)  # [num_layers, batch_size, hidden_size]\n",
    "        c0 = self.init_c(features).unsqueeze(0)  # [num_layers, batch_size, hidden_size]\n",
    "        \n",
    "        # Passa gli embeddings e lo stato nascosto all'LSTM\n",
    "        hiddens, _ = self.lstm(embeddings, (h0, c0))\n",
    "        outputs = self.fc(hiddens)\n",
    "        return outputs\n",
    "    \n",
    "    def sample(self, features, max_len=20):\n",
    "        \"Genera una didascalia data l'immagine\"\n",
    "        sampled_ids = []\n",
    "        \n",
    "        inputs = features.unsqueeze(1)  # [batch_size, 1, embed_size]\n",
    "        h0 = self.init_h(features).unsqueeze(0)\n",
    "        c0 = self.init_c(features).unsqueeze(0)\n",
    "        \n",
    "        states = (h0, c0)\n",
    "        for _ in range(max_len):\n",
    "            hiddens, states = self.lstm(inputs, states)\n",
    "            outputs = self.fc(hiddens.squeeze(1))\n",
    "            _, predicted = outputs.max(1)\n",
    "            sampled_ids.append(predicted.item())\n",
    "            inputs = self.embed(predicted)\n",
    "            inputs = inputs.unsqueeze(1)\n",
    "            if predicted.item() == vocab.word2idx['<END>']:\n",
    "                break\n",
    "        return sampled_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Embedding matrix function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_matrix(vocab, glove_embeddings, embedding_dim):\n",
    "    vocab_size = len(vocab)\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "    for word, idx in vocab.word2idx.items():\n",
    "        if word in glove_embeddings:\n",
    "            embedding_matrix[idx] = glove_embeddings[word]\n",
    "        else:\n",
    "            # Inizializza con un vettore casuale per le parole non trovate\n",
    "            embedding_matrix[idx] = np.random.normal(scale=0.6, size=(embedding_dim,))\n",
    "    return torch.tensor(embedding_matrix, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/white/miniconda3/envs/dml/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/white/miniconda3/envs/dml/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "embed_size = embedding_dim  # Deve corrispondere alla dimensione degli embeddings GloVe\n",
    "hidden_size = 512\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "encoder = EncoderCNN(embed_size).to(device)\n",
    "decoder = DecoderRNN(embed_size, hidden_size, vocab_size, embedding_matrix).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 3e-4\n",
    "\n",
    "for param in decoder.embed.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "decoder_optimizer = torch.optim.Adam([\n",
    "    {'params': decoder.lstm.parameters()},\n",
    "    {'params': decoder.fc.parameters()}\n",
    "], lr=3e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [0/2503], Loss: 8.6817\n",
      "Epoch [1/10], Step [100/2503], Loss: 5.4216\n",
      "Epoch [1/10], Step [200/2503], Loss: 4.9243\n",
      "Epoch [1/10], Step [300/2503], Loss: 4.8657\n",
      "Epoch [1/10], Step [400/2503], Loss: 4.3431\n",
      "Epoch [1/10], Step [500/2503], Loss: 4.4300\n",
      "Epoch [1/10], Step [600/2503], Loss: 4.4037\n",
      "Epoch [1/10], Step [700/2503], Loss: 4.5803\n",
      "Epoch [1/10], Step [800/2503], Loss: 4.4677\n",
      "Epoch [1/10], Step [900/2503], Loss: 3.9984\n",
      "Epoch [1/10], Step [1000/2503], Loss: 3.7531\n",
      "Epoch [1/10], Step [1100/2503], Loss: 4.0147\n",
      "Epoch [1/10], Step [1200/2503], Loss: 3.8801\n",
      "Epoch [1/10], Step [1300/2503], Loss: 3.9744\n",
      "Epoch [1/10], Step [1400/2503], Loss: 3.9489\n",
      "Epoch [1/10], Step [1500/2503], Loss: 3.7777\n",
      "Epoch [1/10], Step [1600/2503], Loss: 3.8650\n",
      "Epoch [1/10], Step [1700/2503], Loss: 3.6426\n",
      "Epoch [1/10], Step [1800/2503], Loss: 3.8890\n",
      "Epoch [1/10], Step [1900/2503], Loss: 3.8811\n",
      "Epoch [1/10], Step [2000/2503], Loss: 3.8033\n",
      "Epoch [1/10], Step [2100/2503], Loss: 3.5844\n",
      "Epoch [1/10], Step [2200/2503], Loss: 3.6648\n",
      "Epoch [1/10], Step [2300/2503], Loss: 3.6604\n",
      "Epoch [1/10], Step [2400/2503], Loss: 3.4992\n",
      "Epoch [1/10], Step [2500/2503], Loss: 3.8323\n",
      "Epoch [2/10], Step [0/2503], Loss: 3.5941\n",
      "Epoch [2/10], Step [100/2503], Loss: 3.4304\n",
      "Epoch [2/10], Step [200/2503], Loss: 3.2976\n",
      "Epoch [2/10], Step [300/2503], Loss: 3.7286\n",
      "Epoch [2/10], Step [400/2503], Loss: 3.2008\n",
      "Epoch [2/10], Step [500/2503], Loss: 3.3467\n",
      "Epoch [2/10], Step [600/2503], Loss: 3.6121\n",
      "Epoch [2/10], Step [700/2503], Loss: 3.5322\n",
      "Epoch [2/10], Step [800/2503], Loss: 3.2679\n",
      "Epoch [2/10], Step [900/2503], Loss: 3.1997\n",
      "Epoch [2/10], Step [1000/2503], Loss: 3.4972\n",
      "Epoch [2/10], Step [1100/2503], Loss: 3.5965\n",
      "Epoch [2/10], Step [1200/2503], Loss: 3.5742\n",
      "Epoch [2/10], Step [1300/2503], Loss: 3.7635\n",
      "Epoch [2/10], Step [1400/2503], Loss: 3.3152\n",
      "Epoch [2/10], Step [1500/2503], Loss: 3.2084\n",
      "Epoch [2/10], Step [1600/2503], Loss: 3.3375\n",
      "Epoch [2/10], Step [1700/2503], Loss: 3.6881\n",
      "Epoch [2/10], Step [1800/2503], Loss: 3.4442\n",
      "Epoch [2/10], Step [1900/2503], Loss: 3.3397\n",
      "Epoch [2/10], Step [2000/2503], Loss: 3.3065\n",
      "Epoch [2/10], Step [2100/2503], Loss: 3.1207\n",
      "Epoch [2/10], Step [2200/2503], Loss: 3.3448\n",
      "Epoch [2/10], Step [2300/2503], Loss: 3.4066\n",
      "Epoch [2/10], Step [2400/2503], Loss: 3.2110\n",
      "Epoch [2/10], Step [2500/2503], Loss: 3.3369\n",
      "Epoch [3/10], Step [0/2503], Loss: 3.2135\n",
      "Epoch [3/10], Step [100/2503], Loss: 3.1376\n",
      "Epoch [3/10], Step [200/2503], Loss: 3.4252\n",
      "Epoch [3/10], Step [300/2503], Loss: 3.1461\n",
      "Epoch [3/10], Step [400/2503], Loss: 3.2340\n",
      "Epoch [3/10], Step [500/2503], Loss: 3.1729\n",
      "Epoch [3/10], Step [600/2503], Loss: 3.0456\n",
      "Epoch [3/10], Step [700/2503], Loss: 3.2367\n",
      "Epoch [3/10], Step [800/2503], Loss: 3.1325\n",
      "Epoch [3/10], Step [900/2503], Loss: 3.2704\n",
      "Epoch [3/10], Step [1000/2503], Loss: 3.1211\n",
      "Epoch [3/10], Step [1100/2503], Loss: 3.2119\n",
      "Epoch [3/10], Step [1200/2503], Loss: 3.3398\n",
      "Epoch [3/10], Step [1300/2503], Loss: 3.0901\n",
      "Epoch [3/10], Step [1400/2503], Loss: 2.9765\n",
      "Epoch [3/10], Step [1500/2503], Loss: 3.4694\n",
      "Epoch [3/10], Step [1600/2503], Loss: 3.3388\n",
      "Epoch [3/10], Step [1700/2503], Loss: 3.4540\n",
      "Epoch [3/10], Step [1800/2503], Loss: 3.3003\n",
      "Epoch [3/10], Step [1900/2503], Loss: 3.1552\n",
      "Epoch [3/10], Step [2000/2503], Loss: 3.2718\n",
      "Epoch [3/10], Step [2100/2503], Loss: 3.2395\n",
      "Epoch [3/10], Step [2200/2503], Loss: 3.0220\n",
      "Epoch [3/10], Step [2300/2503], Loss: 3.0170\n",
      "Epoch [3/10], Step [2400/2503], Loss: 2.9940\n",
      "Epoch [3/10], Step [2500/2503], Loss: 3.0582\n",
      "Epoch [4/10], Step [0/2503], Loss: 3.2008\n",
      "Epoch [4/10], Step [100/2503], Loss: 2.9244\n",
      "Epoch [4/10], Step [200/2503], Loss: 3.1293\n",
      "Epoch [4/10], Step [300/2503], Loss: 3.0441\n",
      "Epoch [4/10], Step [400/2503], Loss: 3.0137\n",
      "Epoch [4/10], Step [500/2503], Loss: 3.1648\n",
      "Epoch [4/10], Step [600/2503], Loss: 3.1465\n",
      "Epoch [4/10], Step [700/2503], Loss: 3.0861\n",
      "Epoch [4/10], Step [800/2503], Loss: 2.8234\n",
      "Epoch [4/10], Step [900/2503], Loss: 2.9346\n",
      "Epoch [4/10], Step [1000/2503], Loss: 3.0685\n",
      "Epoch [4/10], Step [1100/2503], Loss: 3.0857\n",
      "Epoch [4/10], Step [1200/2503], Loss: 3.3050\n",
      "Epoch [4/10], Step [1300/2503], Loss: 3.0644\n",
      "Epoch [4/10], Step [1400/2503], Loss: 2.8839\n",
      "Epoch [4/10], Step [1500/2503], Loss: 2.9486\n",
      "Epoch [4/10], Step [1600/2503], Loss: 2.8505\n",
      "Epoch [4/10], Step [1700/2503], Loss: 2.9948\n",
      "Epoch [4/10], Step [1800/2503], Loss: 3.1206\n",
      "Epoch [4/10], Step [1900/2503], Loss: 3.2228\n",
      "Epoch [4/10], Step [2000/2503], Loss: 2.9458\n",
      "Epoch [4/10], Step [2100/2503], Loss: 3.1286\n",
      "Epoch [4/10], Step [2200/2503], Loss: 3.1794\n",
      "Epoch [4/10], Step [2300/2503], Loss: 3.0110\n",
      "Epoch [4/10], Step [2400/2503], Loss: 3.3288\n",
      "Epoch [4/10], Step [2500/2503], Loss: 3.2845\n",
      "Epoch [5/10], Step [0/2503], Loss: 2.7063\n",
      "Epoch [5/10], Step [100/2503], Loss: 2.6825\n",
      "Epoch [5/10], Step [200/2503], Loss: 2.8896\n",
      "Epoch [5/10], Step [300/2503], Loss: 2.8106\n",
      "Epoch [5/10], Step [400/2503], Loss: 2.8429\n",
      "Epoch [5/10], Step [500/2503], Loss: 2.8380\n",
      "Epoch [5/10], Step [600/2503], Loss: 2.7553\n",
      "Epoch [5/10], Step [700/2503], Loss: 2.8131\n",
      "Epoch [5/10], Step [800/2503], Loss: 2.9741\n",
      "Epoch [5/10], Step [900/2503], Loss: 2.8792\n",
      "Epoch [5/10], Step [1000/2503], Loss: 2.7757\n",
      "Epoch [5/10], Step [1100/2503], Loss: 3.0025\n",
      "Epoch [5/10], Step [1200/2503], Loss: 2.9014\n",
      "Epoch [5/10], Step [1300/2503], Loss: 3.0315\n",
      "Epoch [5/10], Step [1400/2503], Loss: 2.9535\n",
      "Epoch [5/10], Step [1500/2503], Loss: 2.7653\n",
      "Epoch [5/10], Step [1600/2503], Loss: 2.8864\n",
      "Epoch [5/10], Step [1700/2503], Loss: 3.0103\n",
      "Epoch [5/10], Step [1800/2503], Loss: 2.8560\n",
      "Epoch [5/10], Step [1900/2503], Loss: 2.9578\n",
      "Epoch [5/10], Step [2000/2503], Loss: 2.9973\n",
      "Epoch [5/10], Step [2100/2503], Loss: 2.8006\n",
      "Epoch [5/10], Step [2200/2503], Loss: 2.7245\n",
      "Epoch [5/10], Step [2300/2503], Loss: 2.7315\n",
      "Epoch [5/10], Step [2400/2503], Loss: 3.1094\n",
      "Epoch [5/10], Step [2500/2503], Loss: 2.9629\n",
      "Epoch [6/10], Step [0/2503], Loss: 2.7142\n",
      "Epoch [6/10], Step [100/2503], Loss: 2.8712\n",
      "Epoch [6/10], Step [200/2503], Loss: 2.8473\n",
      "Epoch [6/10], Step [300/2503], Loss: 2.8001\n",
      "Epoch [6/10], Step [400/2503], Loss: 2.8911\n",
      "Epoch [6/10], Step [500/2503], Loss: 3.0148\n",
      "Epoch [6/10], Step [600/2503], Loss: 2.7827\n",
      "Epoch [6/10], Step [700/2503], Loss: 2.9722\n",
      "Epoch [6/10], Step [800/2503], Loss: 2.9949\n",
      "Epoch [6/10], Step [900/2503], Loss: 2.8475\n",
      "Epoch [6/10], Step [1000/2503], Loss: 2.7902\n",
      "Epoch [6/10], Step [1100/2503], Loss: 2.7517\n",
      "Epoch [6/10], Step [1200/2503], Loss: 3.0100\n",
      "Epoch [6/10], Step [1300/2503], Loss: 2.8276\n",
      "Epoch [6/10], Step [1400/2503], Loss: 2.6865\n",
      "Epoch [6/10], Step [1500/2503], Loss: 2.7380\n",
      "Epoch [6/10], Step [1600/2503], Loss: 3.0346\n",
      "Epoch [6/10], Step [1700/2503], Loss: 2.7062\n",
      "Epoch [6/10], Step [1800/2503], Loss: 2.6856\n",
      "Epoch [6/10], Step [1900/2503], Loss: 2.8443\n",
      "Epoch [6/10], Step [2000/2503], Loss: 2.6555\n",
      "Epoch [6/10], Step [2100/2503], Loss: 2.8575\n",
      "Epoch [6/10], Step [2200/2503], Loss: 2.8443\n",
      "Epoch [6/10], Step [2300/2503], Loss: 2.7652\n",
      "Epoch [6/10], Step [2400/2503], Loss: 2.8290\n",
      "Epoch [6/10], Step [2500/2503], Loss: 2.4875\n",
      "Epoch [7/10], Step [0/2503], Loss: 2.6850\n",
      "Epoch [7/10], Step [100/2503], Loss: 2.8146\n",
      "Epoch [7/10], Step [200/2503], Loss: 2.8574\n",
      "Epoch [7/10], Step [300/2503], Loss: 2.5810\n",
      "Epoch [7/10], Step [400/2503], Loss: 2.5167\n",
      "Epoch [7/10], Step [500/2503], Loss: 2.7275\n",
      "Epoch [7/10], Step [600/2503], Loss: 2.7004\n",
      "Epoch [7/10], Step [700/2503], Loss: 2.8792\n",
      "Epoch [7/10], Step [800/2503], Loss: 2.7858\n",
      "Epoch [7/10], Step [900/2503], Loss: 2.8198\n",
      "Epoch [7/10], Step [1000/2503], Loss: 2.6884\n",
      "Epoch [7/10], Step [1100/2503], Loss: 2.8921\n",
      "Epoch [7/10], Step [1200/2503], Loss: 2.7987\n",
      "Epoch [7/10], Step [1300/2503], Loss: 2.5604\n",
      "Epoch [7/10], Step [1400/2503], Loss: 2.6399\n",
      "Epoch [7/10], Step [1500/2503], Loss: 2.7509\n",
      "Epoch [7/10], Step [1600/2503], Loss: 2.7182\n",
      "Epoch [7/10], Step [1700/2503], Loss: 2.6543\n",
      "Epoch [7/10], Step [1800/2503], Loss: 2.8984\n",
      "Epoch [7/10], Step [1900/2503], Loss: 2.6670\n",
      "Epoch [7/10], Step [2000/2503], Loss: 2.4284\n",
      "Epoch [7/10], Step [2100/2503], Loss: 2.6437\n",
      "Epoch [7/10], Step [2200/2503], Loss: 2.5934\n",
      "Epoch [7/10], Step [2300/2503], Loss: 2.5621\n",
      "Epoch [7/10], Step [2400/2503], Loss: 2.7739\n",
      "Epoch [7/10], Step [2500/2503], Loss: 2.6795\n",
      "Epoch [8/10], Step [0/2503], Loss: 2.5915\n",
      "Epoch [8/10], Step [100/2503], Loss: 2.6086\n",
      "Epoch [8/10], Step [200/2503], Loss: 2.6883\n",
      "Epoch [8/10], Step [300/2503], Loss: 2.5009\n",
      "Epoch [8/10], Step [400/2503], Loss: 2.7305\n",
      "Epoch [8/10], Step [500/2503], Loss: 2.6436\n",
      "Epoch [8/10], Step [600/2503], Loss: 2.6432\n",
      "Epoch [8/10], Step [700/2503], Loss: 2.9095\n",
      "Epoch [8/10], Step [800/2503], Loss: 2.6558\n",
      "Epoch [8/10], Step [900/2503], Loss: 2.5848\n",
      "Epoch [8/10], Step [1000/2503], Loss: 2.6077\n",
      "Epoch [8/10], Step [1100/2503], Loss: 2.7394\n",
      "Epoch [8/10], Step [1200/2503], Loss: 2.4575\n",
      "Epoch [8/10], Step [1300/2503], Loss: 2.5810\n",
      "Epoch [8/10], Step [1400/2503], Loss: 2.5809\n",
      "Epoch [8/10], Step [1500/2503], Loss: 2.7215\n",
      "Epoch [8/10], Step [1600/2503], Loss: 2.9191\n",
      "Epoch [8/10], Step [1700/2503], Loss: 2.6158\n",
      "Epoch [8/10], Step [1800/2503], Loss: 2.7504\n",
      "Epoch [8/10], Step [1900/2503], Loss: 2.7537\n",
      "Epoch [8/10], Step [2000/2503], Loss: 2.6140\n",
      "Epoch [8/10], Step [2100/2503], Loss: 2.6488\n",
      "Epoch [8/10], Step [2200/2503], Loss: 2.5351\n",
      "Epoch [8/10], Step [2300/2503], Loss: 2.5899\n",
      "Epoch [8/10], Step [2400/2503], Loss: 2.8952\n",
      "Epoch [8/10], Step [2500/2503], Loss: 2.6676\n",
      "Epoch [9/10], Step [0/2503], Loss: 2.6020\n",
      "Epoch [9/10], Step [100/2503], Loss: 2.7684\n",
      "Epoch [9/10], Step [200/2503], Loss: 2.3313\n",
      "Epoch [9/10], Step [300/2503], Loss: 2.5540\n",
      "Epoch [9/10], Step [400/2503], Loss: 2.7135\n",
      "Epoch [9/10], Step [500/2503], Loss: 2.7006\n",
      "Epoch [9/10], Step [600/2503], Loss: 2.7038\n",
      "Epoch [9/10], Step [700/2503], Loss: 2.6652\n",
      "Epoch [9/10], Step [800/2503], Loss: 2.4943\n",
      "Epoch [9/10], Step [900/2503], Loss: 2.5708\n",
      "Epoch [9/10], Step [1000/2503], Loss: 2.5321\n",
      "Epoch [9/10], Step [1100/2503], Loss: 2.7202\n",
      "Epoch [9/10], Step [1200/2503], Loss: 2.6776\n",
      "Epoch [9/10], Step [1300/2503], Loss: 2.4423\n",
      "Epoch [9/10], Step [1400/2503], Loss: 2.4918\n",
      "Epoch [9/10], Step [1500/2503], Loss: 2.6146\n",
      "Epoch [9/10], Step [1600/2503], Loss: 2.5281\n",
      "Epoch [9/10], Step [1700/2503], Loss: 2.7549\n",
      "Epoch [9/10], Step [1800/2503], Loss: 2.4597\n",
      "Epoch [9/10], Step [1900/2503], Loss: 2.6181\n",
      "Epoch [9/10], Step [2000/2503], Loss: 2.6159\n",
      "Epoch [9/10], Step [2100/2503], Loss: 2.3386\n",
      "Epoch [9/10], Step [2200/2503], Loss: 2.6578\n",
      "Epoch [9/10], Step [2300/2503], Loss: 2.7661\n",
      "Epoch [9/10], Step [2400/2503], Loss: 2.5348\n",
      "Epoch [9/10], Step [2500/2503], Loss: 2.3665\n",
      "Epoch [10/10], Step [0/2503], Loss: 2.4875\n",
      "Epoch [10/10], Step [100/2503], Loss: 2.4544\n",
      "Epoch [10/10], Step [200/2503], Loss: 2.4991\n",
      "Epoch [10/10], Step [300/2503], Loss: 2.6706\n",
      "Epoch [10/10], Step [400/2503], Loss: 2.5362\n",
      "Epoch [10/10], Step [500/2503], Loss: 2.4752\n",
      "Epoch [10/10], Step [600/2503], Loss: 2.6000\n",
      "Epoch [10/10], Step [700/2503], Loss: 2.6445\n",
      "Epoch [10/10], Step [800/2503], Loss: 2.4753\n",
      "Epoch [10/10], Step [900/2503], Loss: 2.3330\n",
      "Epoch [10/10], Step [1000/2503], Loss: 2.3730\n",
      "Epoch [10/10], Step [1100/2503], Loss: 2.6885\n",
      "Epoch [10/10], Step [1200/2503], Loss: 2.3968\n",
      "Epoch [10/10], Step [1300/2503], Loss: 2.6289\n",
      "Epoch [10/10], Step [1400/2503], Loss: 2.4888\n",
      "Epoch [10/10], Step [1500/2503], Loss: 2.7536\n",
      "Epoch [10/10], Step [1600/2503], Loss: 2.5724\n",
      "Epoch [10/10], Step [1700/2503], Loss: 2.4441\n",
      "Epoch [10/10], Step [1800/2503], Loss: 2.5581\n",
      "Epoch [10/10], Step [1900/2503], Loss: 2.3158\n",
      "Epoch [10/10], Step [2000/2503], Loss: 2.7004\n",
      "Epoch [10/10], Step [2100/2503], Loss: 2.5028\n",
      "Epoch [10/10], Step [2200/2503], Loss: 2.5651\n",
      "Epoch [10/10], Step [2300/2503], Loss: 2.8373\n",
      "Epoch [10/10], Step [2400/2503], Loss: 2.4611\n",
      "Epoch [10/10], Step [2500/2503], Loss: 2.3284\n"
     ]
    }
   ],
   "source": [
    "def train_model(encoder, decoder, criterion, decoder_optimizer, dataloader, num_epochs):\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "\n",
    "    total_step = len(dataloader)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (images, captions) in enumerate(dataloader):\n",
    "            images = images.to(device)\n",
    "            captions = captions.to(device)\n",
    "\n",
    "            # Imposta i gradienti a zero\n",
    "            decoder_optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            features = encoder(images)\n",
    "            outputs = decoder(features, captions[:, :-1]) \n",
    "\n",
    "            # Calcolo della perdita\n",
    "            loss = criterion(outputs.reshape(-1, outputs.size(2)), captions[:, 1:].reshape(-1))\n",
    "\n",
    "            # Backward pass e ottimizzazione\n",
    "            loss.backward()\n",
    "            decoder_optimizer.step()\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i}/{total_step}], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab.word2idx['<PAD>'])\n",
    "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "train_model(encoder, decoder, criterion, decoder_optimizer, train_loader, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Caption generation and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption(encoder, decoder, image, vocab, max_length=20):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    with torch.no_grad():\n",
    "        image = image.to(device)\n",
    "        feature = encoder(image.unsqueeze(0))\n",
    "        sampled_ids = decoder.sample(feature, max_length)\n",
    "        sampled_caption = []\n",
    "        for word_id in sampled_ids:\n",
    "            word = vocab.idx2word[word_id]\n",
    "            if word == '<END>':\n",
    "                break\n",
    "            sampled_caption.append(word)\n",
    "        sentence = ' '.join(sampled_caption)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image picking\n",
    "test_image, _ = test_dataset[37]\n",
    "plt.imshow(test_image.permute(1, 2, 0))\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# caption generation\n",
    "caption = generate_caption(encoder, decoder, test_image, vocab)\n",
    "print(\"Didascalia Generata:\", caption)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
